{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "228b6968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.5180, 0.4820],\n",
      "         [0.4755, 0.5245]],\n",
      "\n",
      "        [[0.4997, 0.5003],\n",
      "         [0.4944, 0.5056]],\n",
      "\n",
      "        [[0.4919, 0.5081],\n",
      "         [0.4977, 0.5023]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[-0.2245,  0.1684, -0.4920, -0.3165],\n",
      "         [-0.2429,  0.1510, -0.4933, -0.2871]],\n",
      "\n",
      "        [[-0.1365,  0.1325, -0.4653, -0.3849],\n",
      "         [-0.1353,  0.1325, -0.4645, -0.3861]],\n",
      "\n",
      "        [[-0.2538,  0.0532, -0.4771, -0.1982],\n",
      "         [-0.2528,  0.0526, -0.4754, -0.1988]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SelfAttentionV1(nn.Module):\n",
    "    def __init__(self, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.k_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.v_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, X):\n",
    "        Q = self.q_proj(X)\n",
    "        K = self.k_proj(X)\n",
    "        V = self.v_proj(X)\n",
    "\n",
    "        attention_value = Q @ K.transpose(-1, -2) / math.sqrt(self.hidden_dim)\n",
    "        attention_weight = torch.softmax(attention_value, dim=-1)\n",
    "        print(attention_weight)\n",
    "        output = torch.matmul(attention_weight, V)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "X = torch.rand(3, 2, 4)\n",
    "net = SelfAttentionV1(4)\n",
    "my_output = net(X)\n",
    "\n",
    "print(my_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ee41f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4728, 0.5272],\n",
      "         [0.4700, 0.5300]],\n",
      "\n",
      "        [[0.5052, 0.4948],\n",
      "         [0.4900, 0.5100]],\n",
      "\n",
      "        [[0.5311, 0.4689],\n",
      "         [0.5651, 0.4349]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[-0.0595,  0.3484, -0.2286,  0.1935],\n",
      "         [-0.0587,  0.3479, -0.2298,  0.1930]],\n",
      "\n",
      "        [[-0.1897,  0.3886, -0.0556,  0.2985],\n",
      "         [-0.1905,  0.3861, -0.0509,  0.2971]],\n",
      "\n",
      "        [[-0.1905,  0.4036, -0.0889,  0.3350],\n",
      "         [-0.1808,  0.3952, -0.0947,  0.3235]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MySelfAttentionV2(nn.Module):\n",
    "    def __init__(self, hidden_dim: int) -> None:\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.proj = nn.Linear(hidden_dim, hidden_dim * 3)\n",
    "        self.output_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        # X (batch_size, seq_len, hidden_dim)\n",
    "        QKV = self.proj(X)\n",
    "        Q, K, V = torch.split(QKV, self.hidden_dim, -1)\n",
    "        attention_value = Q @ K.transpose(-1, -2) / math.sqrt(self.hidden_dim)\n",
    "        attention_weight = torch.softmax(attention_value, -1)\n",
    "        print(attention_weight)\n",
    "        output = self.output_proj(attention_weight @ V)\n",
    "        return output\n",
    "\n",
    "\n",
    "X = torch.rand(3, 2, 4)\n",
    "net = MySelfAttentionV2(4)\n",
    "output = net(X)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4eb7017e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 4])\n",
      "tensor([[[ 0.0316, -0.5146],\n",
      "         [ 0.1702, -0.2773],\n",
      "         [ 0.0308, -0.5160],\n",
      "         [ 0.2350, -0.2210]],\n",
      "\n",
      "        [[ 0.1761, -0.4255],\n",
      "         [ 0.3927, -0.0598],\n",
      "         [ 0.1761, -0.4256],\n",
      "         [ 0.3878, -0.0675]],\n",
      "\n",
      "        [[ 0.1601, -0.3420],\n",
      "         [ 0.5714,  0.2222],\n",
      "         [ 0.1601, -0.3420],\n",
      "         [ 0.1601, -0.3420]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class MySelfAttentionV3(nn.Module):\n",
    "    def __init__(self, dim: int, dropout_rate: float = 0.1) -> None:\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.attention_dropout = nn.Dropout(dropout_rate)\n",
    "        self.proj = nn.Linear(dim, dim * 3)\n",
    "        self.output_proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(\n",
    "        self, X: torch.Tensor, attention_mask: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        QKV = self.proj(X)\n",
    "        Q, K, V = torch.split(QKV, self.dim, -1)\n",
    "        attention_weight = Q @ K.transpose(-1, -2) / math.sqrt(self.dim)\n",
    "        if attention_mask is not None:\n",
    "            attention_weight = attention_weight.masked_fill(\n",
    "                attention_mask == 0, float(\"-inf\")\n",
    "            )\n",
    "        attention_weight = torch.softmax(attention_weight, -1)\n",
    "        print(attention_weight.shape)\n",
    "\n",
    "        attention_weight = self.attention_dropout(attention_weight)\n",
    "        return self.output_proj(attention_weight @ V)\n",
    "\n",
    "\n",
    "X = torch.rand(3, 4, 2)\n",
    "mask = torch.Tensor([[1, 1, 1, 0], [1, 1, 0, 0], [1, 0, 0, 0]])\n",
    "mask = mask.unsqueeze(1).repeat(1, 4, 1)\n",
    "net = MySelfAttentionV3(2)\n",
    "output = net(X, mask)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "69c6d620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 8, 2, 2])\n",
      "torch.Size([3, 2, 8, 16])\n",
      "torch.Size([3, 2, 128])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim: int, head_num: int, dropout_rate: float) -> None:\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.head_num = head_num\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.proj = nn.Linear(dim, 3 * dim)\n",
    "        self.head_dim = dim // head_num\n",
    "\n",
    "    def forward(self, X: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, seq_len, _ = X.shape\n",
    "        QKV = self.proj(X)\n",
    "        Q, K, V = torch.split(QKV, self.dim, -1)\n",
    "        # (batch_size, seq_len, dim) -> (batch_size, head_num, seq_len, head_dim)\n",
    "        Q = Q.view(batch_size, seq_len, self.head_num, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.head_num, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.head_num, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        attention_weight = Q @ K.transpose(-1, -2) / math.sqrt(self.head_dim)\n",
    "        if attention_mask is not None:\n",
    "            attention_weight = attention_weight.masked_fill(\n",
    "                attention_mask == 0, float(\"-inf\")\n",
    "            )\n",
    "        attention_weight = torch.softmax(attention_weight, -1)\n",
    "        # print(attention_weight)\n",
    "        attention_weight = self.dropout(attention_weight)\n",
    "        output_mid = attention_weight @ V\n",
    "        output_mid = output_mid.transpose(1, 2).contiguous()\n",
    "        print(output_mid.shape)\n",
    "\n",
    "        return output_mid.view(batch_size, seq_len, -1)\n",
    "\n",
    "\n",
    "X = torch.rand(3, 2, 128)\n",
    "net = MultiHeadAttention(128, 8, 0.1)\n",
    "mask = torch.tensor([[1, 0], [0, 1], [1, 1]])\n",
    "# (batch_size, seq_len) -> (batch_size, head_num, seq_len, seq_len)\n",
    "mask = mask.unsqueeze(1).unsqueeze(1).expand(-1, 8, 2, 2)\n",
    "print(mask.shape)\n",
    "output = net(X, mask)\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "165ab257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 128])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class GroupQueryAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, head_num: int, key_value_num: int):\n",
    "        super().__init__()\n",
    "        assert hidden_dim % head_num == 0\n",
    "        assert head_num % key_value_num == 0\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.head_num = head_num\n",
    "        self.key_value_num = key_value_num\n",
    "        self.head_dim = hidden_dim // head_num\n",
    "\n",
    "        self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.k_proj = nn.Linear(hidden_dim, key_value_num * self.head_dim)\n",
    "        self.v_proj = nn.Linear(hidden_dim, key_value_num * self.head_dim)\n",
    "\n",
    "        self.o_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, seq_len, _ = X.size()\n",
    "        q = self.q_proj(X)\n",
    "        k = self.k_proj(X)\n",
    "        v = self.v_proj(X)\n",
    "\n",
    "        # (batch_size, seq_len, hidden_dim) -> (batch_size, head_num, seq_len, head_dim)\n",
    "        q = q.view(batch_size, seq_len, self.head_num, self.head_dim).transpose(1, 2)\n",
    "        # -> (batch_size, key_value_num, seq_len, head_dim)\n",
    "        k = k.view(batch_size, seq_len, self.key_value_num, self.head_dim).transpose(\n",
    "            1, 2\n",
    "        )\n",
    "        v = v.view(batch_size, seq_len, self.key_value_num, self.head_dim).transpose(\n",
    "            1, 2\n",
    "        )\n",
    "        # 扩充 k v\n",
    "        k = k.repeat_interleave(self.head_num // self.key_value_num, 1)\n",
    "        v = v.repeat_interleave(self.head_num // self.key_value_num, 1)\n",
    "\n",
    "        # (batch_size, head_num, seq_len, seq_len)\n",
    "        attention_weight = q @ k.transpose(2, 3) / math.sqrt(self.head_dim)\n",
    "        attention_weight = torch.softmax(attention_weight, -1)\n",
    "        output_mid = attention_weight @ v  # (batch_size, head_num, seq_len, head_dim)\n",
    "        output_mid = (\n",
    "            output_mid.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
    "        )\n",
    "\n",
    "        return self.o_proj(output_mid)\n",
    "\n",
    "\n",
    "X = torch.rand(3, 2, 128)\n",
    "net = GroupQueryAttention(128, 8, 4)\n",
    "output = net(X)\n",
    "print(output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
