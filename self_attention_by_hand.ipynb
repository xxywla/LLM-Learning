{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "228b6968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.5180, 0.4820],\n",
      "         [0.4755, 0.5245]],\n",
      "\n",
      "        [[0.4997, 0.5003],\n",
      "         [0.4944, 0.5056]],\n",
      "\n",
      "        [[0.4919, 0.5081],\n",
      "         [0.4977, 0.5023]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[-0.2245,  0.1684, -0.4920, -0.3165],\n",
      "         [-0.2429,  0.1510, -0.4933, -0.2871]],\n",
      "\n",
      "        [[-0.1365,  0.1325, -0.4653, -0.3849],\n",
      "         [-0.1353,  0.1325, -0.4645, -0.3861]],\n",
      "\n",
      "        [[-0.2538,  0.0532, -0.4771, -0.1982],\n",
      "         [-0.2528,  0.0526, -0.4754, -0.1988]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SelfAttentionV1(nn.Module):\n",
    "    def __init__(self, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.k_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.v_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, X):\n",
    "        Q = self.q_proj(X)\n",
    "        K = self.k_proj(X)\n",
    "        V = self.v_proj(X)\n",
    "\n",
    "        attention_value = Q @ K.transpose(-1, -2) / math.sqrt(self.hidden_dim)\n",
    "        attention_weight = torch.softmax(attention_value, dim=-1)\n",
    "        print(attention_weight)\n",
    "        output = torch.matmul(attention_weight, V)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "X = torch.rand(3, 2, 4)\n",
    "net = SelfAttentionV1(4)\n",
    "my_output = net(X)\n",
    "\n",
    "print(my_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "69c6d620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 8, 2, 2])\n",
      "torch.Size([3, 2, 8, 16])\n",
      "torch.Size([3, 2, 128])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim: int, head_num: int, dropout_rate: float) -> None:\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.head_num = head_num\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.proj = nn.Linear(dim, 3 * dim)\n",
    "        self.head_dim = dim // head_num\n",
    "\n",
    "    def forward(self, X: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, seq_len, _ = X.shape\n",
    "        QKV = self.proj(X)\n",
    "        Q, K, V = torch.split(QKV, self.dim, -1)\n",
    "        # (batch_size, seq_len, dim) -> (batch_size, head_num, seq_len, head_dim)\n",
    "        Q = Q.view(batch_size, seq_len, self.head_num, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.head_num, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.head_num, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        attention_weight = Q @ K.transpose(-1, -2) / math.sqrt(self.head_dim)\n",
    "        if attention_mask is not None:\n",
    "            attention_weight = attention_weight.masked_fill(\n",
    "                attention_mask == 0, float(\"-inf\")\n",
    "            )\n",
    "        attention_weight = torch.softmax(attention_weight, -1)\n",
    "        # print(attention_weight)\n",
    "        attention_weight = self.dropout(attention_weight)\n",
    "        output_mid = attention_weight @ V\n",
    "        output_mid = output_mid.transpose(1, 2).contiguous()\n",
    "        print(output_mid.shape)\n",
    "\n",
    "        return output_mid.view(batch_size, seq_len, -1)\n",
    "\n",
    "\n",
    "X = torch.rand(3, 2, 128)\n",
    "net = MultiHeadAttention(128, 8, 0.1)\n",
    "mask = torch.tensor([[1, 0], [0, 1], [1, 1]])\n",
    "# (batch_size, seq_len) -> (batch_size, head_num, seq_len, seq_len)\n",
    "mask = mask.unsqueeze(1).unsqueeze(1).expand(-1, 8, 2, 2)\n",
    "print(mask.shape)\n",
    "output = net(X, mask)\n",
    "print(output.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
