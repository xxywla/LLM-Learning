{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "063b5ed9",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "MODEL_NAME = \"Qwen/Qwen3-0.6B\"\n",
        "\n",
        "# 加载模型及其分词器\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"cpu\",\n",
        "    dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "e4488666",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 785, 6722,  315, 9625,  374]])\n"
          ]
        }
      ],
      "source": [
        "prompt = 'The capital of France is'\n",
        "\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "print(input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "63605fe8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.decode(785))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "880e817b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BaseModelOutputWithPast(last_hidden_state=tensor([[[ 7.6875, 18.8750, -0.0466,  ..., -1.2266,  1.1797,  1.5000],\n",
            "         [ 3.5156, -9.3750, -1.1641,  ...,  0.0204,  0.5977, -1.5312],\n",
            "         [-0.8594,  3.4062, -1.3047,  ..., -0.7500, -2.3594,  0.9688],\n",
            "         [ 0.6133, -0.3672, -1.2031,  ...,  0.2344, -0.8164, -0.6914],\n",
            "         [-2.6875,  6.2812, -1.0859,  ...,  1.4922, -3.1250, -1.3906]]],\n",
            "       dtype=torch.bfloat16, grad_fn=<MulBackward0>), past_key_values=DynamicCache(layers=[DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer]), hidden_states=None, attentions=None)\n"
          ]
        }
      ],
      "source": [
        "output_ids = model.model(input_ids)\n",
        "print(output_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "65511aac",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'>\n",
            "<class 'transformers.cache_utils.DynamicCache'>\n"
          ]
        }
      ],
      "source": [
        "print(type(output_ids[0]))\n",
        "print(type(output_ids[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "983a55bf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 5, 1024])\n",
            "torch.Size([1, 5, 151936])\n",
            "tensor(12095)\n",
            " Paris\n"
          ]
        }
      ],
      "source": [
        "print(output_ids[0].shape)\n",
        "lm_head = model.lm_head(output_ids[0])\n",
        "print(lm_head.shape)\n",
        "token_id = lm_head[0, -1].argmax(-1)\n",
        "print(token_id)\n",
        "print(tokenizer.decode(token_id))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "6b1ae217",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(15846)\n",
            " Question\n",
            "tensor(315)\n",
            " of\n",
            "tensor(279)\n",
            " the\n",
            "tensor(374)\n",
            " is\n",
            "tensor(12095)\n",
            " Paris\n"
          ]
        }
      ],
      "source": [
        "for idx in range(len(lm_head[0])):\n",
        "    token_id = lm_head[0, idx].argmax(-1)\n",
        "    print(token_id)\n",
        "    print(tokenizer.decode(token_id))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "fc72aea5",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[  785,  6722,   315,  9625,   374, 12095,    13,   576,  6722,   315,\n",
            "          8359,   374, 22415,    13,   576,  6722,   315, 15344,   374, 21718,\n",
            "            13,   576,  6722,   315,  5616]])\n"
          ]
        }
      ],
      "source": [
        "output = model.generate(input_ids=input_ids)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "943ba984",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The capital of France is Paris. The capital of Russia is Moscow. The capital of Italy is Rome. The capital of China\n"
          ]
        }
      ],
      "source": [
        "generated_text = tokenizer.decode(output[0])\n",
        "print(generated_text)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
